{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPmn2UAAjvfUGh1DPe5ods5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daniel-falk/ai-ml-principles-exercises/blob/main/ML-training/intro-to-libraries/intro_to_sklearn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning with sklearn\n",
        "Scikit-learn is a machine learning library that has lots of traditional machine learning algorithms but also many useful functions for all the other tasks in machine learning code:\n",
        "\n",
        "* ML algorithms\n",
        "* Sample datasets\n",
        "* Data preparation functions\n",
        "* Evaluation metrics and functions\n",
        "\n",
        "The library and project is called scikit-learn but the python module is named `sklearn`.\n",
        "\n",
        "## Line fitting\n",
        "To start with we are going to create a fake dataset and fit a line to the data points. This can be useful when trying to predict the price of some asset given some parameters, the weight of an object, the age of a person or how satisfied a customer is with a service."
      ],
      "metadata": {
        "id": "jijgspvoflzv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIJ_nmhiRNHR"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.linspace(start=0, stop=2*np.pi, num=20)\n",
        "y_sin = np.sin(x)  # sesonal variation component\n",
        "y = 0.1 * x + 0.05 * y_sin\n",
        "\n",
        "plt.plot(x, y, \"*\", color=\"green\")\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('f(x)')\n",
        "plt.title(\"A fake dataset\")"
      ],
      "metadata": {
        "id": "TIxD_PBziXvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import linear_model\n",
        "\n",
        "regressor = linear_model.LinearRegression()\n",
        "regressor.fit(np.expand_dims(x, axis=1), y)"
      ],
      "metadata": {
        "id": "jyxm72L9hvFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = regressor.predict(np.expand_dims(x, axis=1))\n",
        "\n",
        "plt.plot(x, y, \"*\", color=\"green\", label=\"training dataset\")\n",
        "plt.plot(x, predicted, color=\"red\", label=\"predicted\")\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('f(x)')\n",
        "plt.title(\"Predicted vs. true\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "8fC2cUDZkywD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be seen the linear function tries to fit a straight line to a function which is not straight. At some points we have an error between the predicted value and the true value."
      ],
      "metadata": {
        "id": "FhB_gOzBnZh5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"True value at x=2 is {y[2]:.4f} but predicted value is {predicted[2]:.4f}\")"
      ],
      "metadata": {
        "id": "wQUUTSSBnYdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can calculate the mean (squared) error over the full training dataset"
      ],
      "metadata": {
        "id": "PwXIefUZoC8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "print(f\"MSE: {mean_squared_error(y, predicted)}\")\n",
        "print(f\"R² score: {r2_score(y, predicted) * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "1u6b3p3bn-4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we see that it is not possible to fit a linear function to our data, let's try some polynomial. Instead of using a polynomial regressor, we can use the linear regressor with polynomials of the dataset as input. Lets consider the following case where we transform 5 data points to 3rd order polynomials, that is:\n",
        "$\\{a\\} → \\{1, a, a^2, a^3\\}$"
      ],
      "metadata": {
        "id": "Hd6fHrH1qWAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "to_polynom = PolynomialFeatures(3)\n",
        "to_polynom.fit_transform(np.expand_dims(np.array([1,2,3,4,5]), axis=1))"
      ],
      "metadata": {
        "id": "XNKsd5ePpYtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_3rd_order = to_polynom.fit_transform(np.expand_dims(x, axis=1))\n",
        "regressor_3rd_order = linear_model.LinearRegression()\n",
        "regressor_3rd_order.fit(x_3rd_order, y)"
      ],
      "metadata": {
        "id": "eI8zrcQ2rh45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_3rd_order = regressor_3rd_order.predict(x_3rd_order)\n",
        "\n",
        "plt.plot(x, y, \"*\", color=\"green\", label=\"training dataset\")\n",
        "plt.plot(x, predicted_3rd_order, color=\"red\", label=\"predicted (3rd order)\")\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('f(x)')\n",
        "plt.title(\"Predicted vs. true\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "4aLW2CeRr98z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"3rd order prediction MSE: {mean_squared_error(y, predicted_3rd_order)}\")\n",
        "print(f\"3rd order prediction R² score: {r2_score(y, predicted_3rd_order) * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "L8ixl5lUsq0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now see that the function fits very well to the training data, but how about data outside of this interval? We can use the same function as before to create a larger dataset by only adding more x-values in a larger interval."
      ],
      "metadata": {
        "id": "RlZZkKhEs5gD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_large = np.linspace(start=-10, stop=10, num=20)\n",
        "y_large_sin = np.sin(x_large)  # sesonal variation component\n",
        "y_large = 0.1 * x_large + 0.05 * y_large_sin\n",
        "\n",
        "plt.plot(x_large, y_large, \"*\", color=\"green\")\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('f(x)')\n",
        "plt.title(\"A larger fake dataset\")"
      ],
      "metadata": {
        "id": "VI7UqGDGtOyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_3rd_order = to_polynom.fit_transform(np.expand_dims(x_large, axis=1))\n",
        "predicted_3rd_order = regressor_3rd_order.predict(x_3rd_order)\n",
        "\n",
        "plt.plot(x_large, y_large, \"*\", color=\"green\", label=\"training dataset\")\n",
        "plt.plot(x_large, predicted_3rd_order, color=\"red\", label=\"predicted (3rd order)\")\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('f(x)')\n",
        "plt.title(\"Predicted vs. true\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "Qists3octnfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can be seen that the model only fits well within the range it was trained on ($0-2\\pi$), i.e. it is good at interpolating but not at extrapolating.\n",
        "\n",
        "We did manually select the polynomial to be a 3rd grade polynomial (this is called a hyperparameter), is that model able to fit all kinds of training data?\n",
        "\n"
      ],
      "metadata": {
        "id": "HNaUA_9Vtwhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x2 = np.linspace(start=0, stop=2*np.pi, num=40)\n",
        "y2_sin = np.sin(x2)\n",
        "y2_cos = np.cos(x2)\n",
        "y2 = np.maximum(y2_sin, y2_cos)\n",
        "\n",
        "plt.plot(x2, y2, \"*\", color=\"green\")\n",
        "plt.xlabel('x2')\n",
        "plt.ylabel('f(x2)')\n",
        "plt.title(\"A fake dataset\")"
      ],
      "metadata": {
        "id": "JPd7-uizuqmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a new linear regressor\n",
        "regressor = linear_model.LinearRegression()\n",
        "regressor.fit(np.expand_dims(x2, axis=1), y2)\n",
        "predicted = regressor.predict(np.expand_dims(x2, axis=1))\n",
        "\n",
        "# Train a new 3rd order polynomial regressor\n",
        "x2_3rd_order = to_polynom.fit_transform(np.expand_dims(x2, axis=1))\n",
        "regressor_3rd_order = linear_model.LinearRegression()\n",
        "regressor_3rd_order.fit(x2_3rd_order, y2)\n",
        "predicted_3rd_order = regressor_3rd_order.predict(x2_3rd_order)\n",
        "\n",
        "plt.plot(x2, y2, \"*\", color=\"green\", label=\"training dataset\")\n",
        "plt.plot(x2, predicted, color=\"red\", label=\"linear\")\n",
        "plt.plot(x2, predicted_3rd_order, color=\"blue\", label=\"3rd order\")\n",
        "plt.xlabel('x2')\n",
        "plt.ylabel('f(x2)')\n",
        "plt.legend()\n",
        "plt.title(\"Predicted vs. true\")"
      ],
      "metadata": {
        "id": "I8_t41UzvdGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"linear prediction MSE: {mean_squared_error(y2, predicted)}\")\n",
        "print(f\"linear prediction R² score: {r2_score(y2, predicted) * 100:.2f}%\")\n",
        "print(f\"3rd order prediction MSE: {mean_squared_error(y2, predicted_3rd_order)}\")\n",
        "print(f\"3rd order prediction R² score: {r2_score(y2, predicted_3rd_order) * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "hGmyVzbvwYuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can we do better? Some ML algorithms are more powerful and capable of fitting a more diverse set of functions. Let's try with a *Support Vector Regressor*."
      ],
      "metadata": {
        "id": "CO4JQeiYwxRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVR\n",
        "\n",
        "fig, axs = plt.subplots(2)\n",
        "\n",
        "# Fit an SVR to the new dataset\n",
        "regressor = SVR()\n",
        "regressor.fit(np.expand_dims(x2, axis=1), y2)\n",
        "predicted = regressor.predict(np.expand_dims(x2, axis=1))\n",
        "\n",
        "axs[0].plot(x2, y2, \"*\", color=\"green\", label=\"training dataset\")\n",
        "axs[0].plot(x2, predicted, color=\"red\", label=\"SVR\")\n",
        "axs[0].set_xlabel('x2')\n",
        "axs[0].set_ylabel('f(x2)')\n",
        "axs[0].legend()\n",
        "axs[0].set_title(\"Predicted vs. true: new dataset\")\n",
        "\n",
        "# Fit an SVR to the first dataset\n",
        "regressor = SVR()\n",
        "regressor.fit(np.expand_dims(x, axis=1), y)\n",
        "predicted = regressor.predict(np.expand_dims(x, axis=1))\n",
        "\n",
        "axs[1].plot(x, y, \"*\", color=\"green\", label=\"training dataset\")\n",
        "axs[1].plot(x, predicted, color=\"red\", label=\"SVR\")\n",
        "axs[1].set_xlabel('x')\n",
        "axs[1].set_ylabel('f(x)')\n",
        "axs[1].legend()\n",
        "axs[1].set_title(\"Predicted vs. true: first dataset\")\n",
        "\n",
        "plt.gcf().tight_layout() # fix margin between subplots"
      ],
      "metadata": {
        "id": "-TzguUvxwwXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data classification with sklearn\n",
        "Next we are going to create an artificial 2-dimensional dataset with two clusters. We're going to use the [Support Vector Machine](https://scikit-learn.org/stable/modules/svm.html) implemented in scikit-learn and train that to differentiate data from the two clusters."
      ],
      "metadata": {
        "id": "b5m2_Jz3htpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a fake dataset\n",
        "num_dots = 100\n",
        "data = np.random.random(size=(num_dots, 2))\n",
        "labels = np.random.random(size=num_dots) > 0.5\n",
        "\n",
        "data[labels,:] += 0.7  # Make the \"True\" samples different i x and y\n",
        "\n",
        "plt.title(\"Fake dataset\")\n",
        "plt.scatter(data[:,0], data[:,1], c=labels, alpha=0.3, cmap='viridis')"
      ],
      "metadata": {
        "id": "x-KEzdrORZNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "\n",
        "classifier = svm.SVC() # Create the SVM model\n",
        "classifier.fit(data, labels) # train the model"
      ],
      "metadata": {
        "id": "AGDoU-FKSIcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = classifier.predict(data)\n",
        "correct = predicted == labels\n",
        "\n",
        "edge_colors = np.zeros(shape=(len(labels), 3))\n",
        "edge_colors[~correct] = (1, 0, 0)\n",
        "\n",
        "plt.title(\"Training data predicted\")\n",
        "plt.scatter(data[:,0], data[:,1], c=predicted, edgecolors=edge_colors, alpha=0.6, cmap='viridis')"
      ],
      "metadata": {
        "id": "Ab6DDCX4Ssa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create more random data and predict labels\n",
        "num_dots = 1000\n",
        "random_data = np.random.random(size=(num_dots, 2))\n",
        "random_data *= 1.7\n",
        "\n",
        "plt.title(\"Predicted random data\")\n",
        "plt.scatter(random_data[:,0], random_data[:,1], c=classifier.predict(random_data), alpha=0.3, cmap='viridis')"
      ],
      "metadata": {
        "id": "AfYLOG6aS7Dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also use the `metrics` module in `sklearn` to measure how many of the data points in the training data that got correctly classified by the classifier."
      ],
      "metadata": {
        "id": "ynDOauewz404"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
        "\n",
        "print(classification_report(labels, predicted))"
      ],
      "metadata": {
        "id": "tuXDQo1DhOA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ConfusionMatrixDisplay.from_predictions(labels, predicted)"
      ],
      "metadata": {
        "id": "pRqGUmWjz0GZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do however note that the error measured above on the training dataset is not the error we can expect to see when the model is used in production. This is since we are testing on the training dataset, the model can memorize the training dataset without having any understanding or capability of generalizing.\n",
        "\n",
        "What we measured above is the \"bias error\", which is an indication on how capable the model is to fit the dataset. A linear classifier cannot correctly classify the data points in two overlapping clusters since the decision boundary is always a straight line, it will thus have an high \"bias error\".\n",
        "\n",
        "Other types of error is \"variance error\" which indicates how much difference there are in two models trained on two different subsets of the same data. I.e. \"variance error\" is high for models that generalizes badly."
      ],
      "metadata": {
        "id": "AGIUHCaZ1cA7"
      }
    }
  ]
}
